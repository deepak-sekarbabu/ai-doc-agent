# AI Documentation Agent - Environment Configuration

# ===========================
# Ollama API Configuration
# ===========================

# Choose between 'cloud' or 'local' Ollama instance
OLLAMA_MODE=cloud

# URL for Ollama API endpoint (auto-configured based on OLLAMA_MODE above)
# Cloud mode: https://ollama.com/api/generate
# Local mode: http://localhost:11434/api/generate
OLLAMA_API_URL=https://ollama.com/api/generate

# API Key (optional - only if your Ollama instance requires authentication)
OLLAMA_API_KEY=

# Default model to use for documentation generation
# Cloud models: gpt-oss:120b-cloud, llama2:70b-chat, codellama:34b
# Local models: llama2:7b, mistral:7b, codellama:7b, phi:2.7b
MODEL_NAME=gpt-oss:120b-cloud

# API request timeout in seconds
API_TIMEOUT=300

# ===========================
# Agent Behavior Configuration
# ===========================

# Maximum number of retry attempts for failed API calls
MAX_RETRIES=3

# Base delay in seconds between retries (uses exponential backoff)
RETRY_DELAY=2

# Enable response caching to avoid redundant API calls
ENABLE_CACHING=true

# Cache directory (relative to project root)
CACHE_DIR=.cache

# Maximum age of cached entries in hours
CACHE_MAX_AGE_HOURS=24

# Maximum number of cached entries
CACHE_MAX_ENTRIES=100

# Critique threshold for accepting documentation (0.0-1.0)
CRITIQUE_THRESHOLD=0.8

# ===========================